{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower Classification\n",
    "\n",
    "In this project I will be participating in the [Petals to the Metal](https://www.kaggle.com/c/tpu-getting-started/overview) competition, which is a \"getting started\" competition on [Kaggle](kaggle.com). The objective is to classify different flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### GPU Setup\n",
    "This cell is there to let me use my small gpu (NVIDIA 1050ti) or my cpu to run Tensorflow. I only use my big GPU if I need it, because Tensorflow can cause issues with other programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12269984462304379811\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9104897474\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6616172957136425047\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:26:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3135687884\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17530803730590975267\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:25:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import os\n",
    "os.environ['TF_MIN_GPU_MULTIPROCESSOR_COUNT']='4'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math, re, os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#from sklearn.\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "IMAGE_SIZE = [512, 512]\n",
    "#BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU Strategy\n",
    "\n",
    "Using TPUs requires the use of an explicit distribution strategy. The workload needs to be distributed equally between all of the TPU cores.\n",
    "\n",
    "The first step is to find out if a TPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU', tpu.master)\n",
    "except ValueError:\n",
    "        tpu = None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next if a TPU is available, a TPU distribution strategy has to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "if tpu:\n",
    "    tf.config.experimental.connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.diestibute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('REPLICAS: ' , strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'.\\tfrecords-jpeg-512x512'\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(path + '/train/*.tfrec')\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(path + '/val/*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob(path + '/test/*.tfrec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "These functions can be found in the `petal_helper.py` file. I will be re-making them here in an effort to understand what they do. These functions are primarily used to prepare the data for use by the TPU. With a TPU it is important to always keep it supplied with data to analyze, for this reason, the data is in the form of `tfrec` files, as opposed to the `jpeg` pictures and separate `csv` with the labels for the pictures. This data is divided into 16 `tfrec` files, meaning that they contain both the pictures and the labels if applicable (The holdout set contains the unique identifiers of the pictures). The TPU that is used has 8 distinct cores. It is recommended to split the dataset up into twice the number of TPU cores to maximize efficiency. In order to minimize loading times, the datasets are also loaded into \"close proximity\" to the TPU, meaning in the same bucket in google cloud services in this case. It will be interesting if data in the same configuration can be used on my gpus and if not what changes I will have to make to adapt the code. I would expect the code to possibly run smoothly on my big GPU (2080ti) and cause issues with my small GPU (1050ti) due to the much lower memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the datasets from the `tfrec` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    '''Turn image data into an array of numbers\n",
    "    Args:\n",
    "        image_data: jpeg image extracted from a tfrecord file\n",
    "    Returns:\n",
    "        tensor containing the data of the image\n",
    "        \n",
    "    '''\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # convert image to floates in [0,1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE , 3]) # explicit size needed for TPU\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string), # tf.string means bytesting\n",
    "        'class': tf.io.FixedLenFeature([], tf.int64) # shape[] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'id': tf.io.FixedLenFeature([], tf.string)\n",
    "        # no class, because this will be used on the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image, idnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, labeled = True, ordered = False):\n",
    "    # Read from tfrecords. for optimal performance, reading from multiple files at once and disregarding\n",
    "    # data order. order does not mapper since we will be shuffling the data anyway\n",
    "    # it's generally a good idea to split the data into 16 parts when working with a tpu\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False #disable order use what is currently being loaded\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) \n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord,\n",
    "                          num_parallel_calls = AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled = True or (image, id) if not labeled\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset shapes: ((512, 512, 3), ()), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(TRAINING_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmenting the data\n",
    "\n",
    "In order to reduce overfitting a function will be created that will flip the images randomly. This could be \"augmented\" by additional operations like cutting or rotating the images later on to possibly increase accuracy.\n",
    "\n",
    "The `dataset.prefetch(AUTO)` in the following `get_training_dataset` function causes all of the data pipeline code to be execute on the cpu during gradient descent calculations on the TPU, this should cause the performance to not be impacted. It will be interesting to see if this also applies while running the code on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Datasets for Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled = True)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.repeat() #This makes the dataset repeat if needed to always fully utilize the TPU\n",
    "    dataset = dataset.shuffle(2048) #This shuffles the order of the datapoints in the dataset to combat overfitting\n",
    "    dataset = dataset.batch(BATCH_SIZE) # This sets the batch size used, probably needs to be adjusted wehn usingt the gpu\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next patch while trining (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered = False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled = True, ordered = ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered = False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled = False, ordered = ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "These are all the classes of flower that are contained in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  <PrefetchDataset shapes: ((None, 512, 512, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "Validation:  <PrefetchDataset shapes: ((None, 512, 512, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "Test:  <PrefetchDataset shapes: ((None, 512, 512, 3), (None,)), types: (tf.float32, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "ds_train = get_training_dataset()\n",
    "ds_valid = get_validation_dataset()\n",
    "ds_test = get_test_dataset()\n",
    "\n",
    "print('Training: ', ds_train)\n",
    "print('Validation: ', ds_valid)\n",
    "print('Test: ', ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n",
    "           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n",
    "           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n",
    "           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n",
    "           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n",
    "           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n",
    "           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n",
    "           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n",
    "           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n",
    "           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n",
    "           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 104\n",
      "First 5 classes, alphabetical:\n",
      "alpine sea holly\n",
      "anthurium\n",
      "artichoke\n",
      "azalea\n",
      "balloon flower\n"
     ]
    }
   ],
   "source": [
    "print('Number of classes: {}'.format(len(CLASSES)))\n",
    "print('First 5 classes, alphabetical:')\n",
    "for name in sorted(CLASSES)[:5]:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Count the Number of Images\n",
    "\n",
    "To find out how many images are in a given dataset, the individual filenames can be used. An example of a filename is `00-512x512-798.tfrec`. The number 798 in the name indicates that the file contains 798 images. For this reason regular expressions can be used to extract this number and then sum up all the individual numbers to get a total for a given dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    n = np.sum([int(re.compile(r'-(\\d*)\\.').search(filename).group(1)) for filename in filenames])\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deep Learning Models\n",
    "### VGG16 based model\n",
    "\n",
    "Now a first version of the deep learning model will be set up. This model will consist of a base made up of a VGG16 model trained on the ImageNet dataset. This base will then be used in transfer learning by adding additional layers and training them on the dataset of flowers. The ImageNet model will not be trained but used as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 16, 16, 512)       14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 104)               53352     \n",
      "=================================================================\n",
      "Total params: 14,768,040\n",
      "Trainable params: 53,352\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = [*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "         # Transfer learning using ImageNet as a base to extract features from the images\n",
    "         pretrained_model,\n",
    "         # attach a new head to act as a classifier\n",
    "         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "         tf.keras.layers.Dense(len(CLASSES), activation = 'softmax')\n",
    "     ])\n",
    "    model.compile(\n",
    "            optimizer = 'adam',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['sparse_categorical_accuracy']\n",
    "            \n",
    "        )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 797 steps\n",
      "Epoch 1/12\n",
      "797/797 [==============================] - 184s 231ms/step - loss: 3.7607 - sparse_categorical_accuracy: 0.1688 - val_loss: 3.3967 - val_sparse_categorical_accuracy: 0.2586\n",
      "Epoch 2/12\n",
      "797/797 [==============================] - 173s 217ms/step - loss: 3.1749 - sparse_categorical_accuracy: 0.2771 - val_loss: 3.0059 - val_sparse_categorical_accuracy: 0.3249\n",
      "Epoch 3/12\n",
      "797/797 [==============================] - 180s 226ms/step - loss: 2.8386 - sparse_categorical_accuracy: 0.3432 - val_loss: 2.7519 - val_sparse_categorical_accuracy: 0.3677\n",
      "Epoch 4/12\n",
      "797/797 [==============================] - 177s 222ms/step - loss: 2.6042 - sparse_categorical_accuracy: 0.4025 - val_loss: 2.5421 - val_sparse_categorical_accuracy: 0.4162\n",
      "Epoch 5/12\n",
      "797/797 [==============================] - 201s 252ms/step - loss: 2.4313 - sparse_categorical_accuracy: 0.4413 - val_loss: 2.3980 - val_sparse_categorical_accuracy: 0.4426\n",
      "Epoch 6/12\n",
      "797/797 [==============================] - 197s 248ms/step - loss: 2.2787 - sparse_categorical_accuracy: 0.4817 - val_loss: 2.2795 - val_sparse_categorical_accuracy: 0.4833\n",
      "Epoch 7/12\n",
      "797/797 [==============================] - 198s 248ms/step - loss: 2.1632 - sparse_categorical_accuracy: 0.5051 - val_loss: 2.1745 - val_sparse_categorical_accuracy: 0.5054\n",
      "Epoch 8/12\n",
      "797/797 [==============================] - 199s 250ms/step - loss: 2.0472 - sparse_categorical_accuracy: 0.5335 - val_loss: 2.0850 - val_sparse_categorical_accuracy: 0.5154\n",
      "Epoch 9/12\n",
      "797/797 [==============================] - 180s 226ms/step - loss: 1.9892 - sparse_categorical_accuracy: 0.5462 - val_loss: 2.0153 - val_sparse_categorical_accuracy: 0.5374\n",
      "Epoch 10/12\n",
      "797/797 [==============================] - 178s 223ms/step - loss: 1.8911 - sparse_categorical_accuracy: 0.5705 - val_loss: 1.9429 - val_sparse_categorical_accuracy: 0.5461\n",
      "Epoch 11/12\n",
      "797/797 [==============================] - 197s 247ms/step - loss: 1.8378 - sparse_categorical_accuracy: 0.5771 - val_loss: 1.8998 - val_sparse_categorical_accuracy: 0.5609\n",
      "Epoch 12/12\n",
      "797/797 [==============================] - 199s 250ms/step - loss: 1.7784 - sparse_categorical_accuracy: 0.5970 - val_loss: 1.8311 - val_sparse_categorical_accuracy: 0.5892\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "STEPS_PER_EPOCH = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data = ds_valid,\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a Submission\n",
    "\n",
    "Now I will use the model to make a submission to the Kaggle competition in order to judge its efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 67  28  95 ...  48 102  62]\n"
     ]
    }
   ],
   "source": [
    "test_ds = get_test_dataset(ordered = True)\n",
    "\n",
    "test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "probabilities = model.predict(test_images_ds)\n",
    "predictions = np.argmax(probabilities, axis = -1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Get image ids from test set and convert to ints\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
    "# Write the submission file\n",
    "np.savetxt(\n",
    "    'submission.csv',\n",
    "    np.rec.fromarrays([test_ids, predictions]),\n",
    "    fmt = ['%s', '%d'],\n",
    "    delimiter = ',',\n",
    "    header = 'id, label',\n",
    "    comments = '',\n",
    ")\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception based model\n",
    "\n",
    "Now instead of using the VGG16 model as a base to be re-trained, I will be using the Xception model, which should be better suited for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 17s 0us/step\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "xception (Model)             (None, 16, 16, 2048)      20861480  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 104)               213096    \n",
      "=================================================================\n",
      "Total params: 21,074,576\n",
      "Trainable params: 213,096\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope(): \n",
    "    pretrained_model = tf.keras.applications.Xception(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = [*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "         # Transfer learning using ImageNet as a base to extract features from the images\n",
    "         pretrained_model,\n",
    "         # attach a new head to act as a classifier\n",
    "         tf.keras.layers.GlobalAveragePooling2D(),\n",
    "         tf.keras.layers.Dense(len(CLASSES), activation = 'softmax')\n",
    "     ])\n",
    "    model.compile(\n",
    "            optimizer = 'adam',\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['sparse_categorical_accuracy']\n",
    "            \n",
    "        )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 797 steps\n",
      "Epoch 1/12\n",
      "797/797 [==============================] - 178s 224ms/step - loss: 2.6241 - sparse_categorical_accuracy: 0.4750 - val_loss: 2.4995 - val_sparse_categorical_accuracy: 0.3823\n",
      "Epoch 2/12\n",
      "797/797 [==============================] - 174s 219ms/step - loss: 1.3561 - sparse_categorical_accuracy: 0.7272 - val_loss: 2.3093 - val_sparse_categorical_accuracy: 0.4221\n",
      "Epoch 3/12\n",
      "797/797 [==============================] - 175s 220ms/step - loss: 0.9947 - sparse_categorical_accuracy: 0.7888 - val_loss: 2.2881 - val_sparse_categorical_accuracy: 0.4146\n",
      "Epoch 4/12\n",
      "797/797 [==============================] - 176s 221ms/step - loss: 0.8168 - sparse_categorical_accuracy: 0.8214 - val_loss: 2.2524 - val_sparse_categorical_accuracy: 0.4353\n",
      "Epoch 5/12\n",
      "797/797 [==============================] - 157s 197ms/step - loss: 0.6883 - sparse_categorical_accuracy: 0.8454 - val_loss: 2.2097 - val_sparse_categorical_accuracy: 0.4512\n",
      "Epoch 6/12\n",
      "797/797 [==============================] - 175s 220ms/step - loss: 0.6183 - sparse_categorical_accuracy: 0.8594 - val_loss: 2.1979 - val_sparse_categorical_accuracy: 0.4526\n",
      "Epoch 7/12\n",
      "797/797 [==============================] - 177s 222ms/step - loss: 0.5529 - sparse_categorical_accuracy: 0.8698 - val_loss: 2.2035 - val_sparse_categorical_accuracy: 0.4690\n",
      "Epoch 8/12\n",
      "797/797 [==============================] - 181s 227ms/step - loss: 0.5112 - sparse_categorical_accuracy: 0.8767 - val_loss: 2.2987 - val_sparse_categorical_accuracy: 0.4485\n",
      "Epoch 9/12\n",
      "797/797 [==============================] - 176s 221ms/step - loss: 0.4755 - sparse_categorical_accuracy: 0.8869 - val_loss: 2.2158 - val_sparse_categorical_accuracy: 0.4615\n",
      "Epoch 10/12\n",
      "797/797 [==============================] - 175s 220ms/step - loss: 0.4361 - sparse_categorical_accuracy: 0.8949 - val_loss: 2.4001 - val_sparse_categorical_accuracy: 0.4407\n",
      "Epoch 11/12\n",
      "797/797 [==============================] - 175s 220ms/step - loss: 0.4135 - sparse_categorical_accuracy: 0.8967 - val_loss: 2.2015 - val_sparse_categorical_accuracy: 0.4709\n",
      "Epoch 12/12\n",
      "797/797 [==============================] - 175s 220ms/step - loss: 0.3854 - sparse_categorical_accuracy: 0.9043 - val_loss: 2.2375 - val_sparse_categorical_accuracy: 0.4647\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "STEPS_PER_EPOCH = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data = ds_valid,\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
